Tasks:

- Take off-the-shelf linear model to make the classification and have it as a form of baseline

- Finish implementing paper

- If time permits it: look into bayesian approaches

Questions for Michael:

- Check if batch norm is performed correctly


Hypotheses for overfitting:
 - Dropout not deactivated in eval mode -> should be covered with model.eval() - try also setting inplace=False in dropouts (just in case)
 - Too small batchsize
 - Data loading could be done incorrectly?
 - Could be that there's class imbalance between train and test
 - Keep 21 classes 


Implement Deep ensemble followed by SWAG   + Look at accuraty and calibration once done
Save for 50 models

Separate in a different function the inference, and then build a conventional test and swag multi-swag test.

compute LPD and expected calibration error (uncertainty toolbox might help). https://github.com/uncertainty-toolbox/uncertainty-toolbox 

Merge all the fold plots to a single one with average value and error bars (standard error = standard dev. / sqrt(N folds))

Confidence metric or entropy of the distribution in order to account for uncertainty.

In Average fold plot make points for individual models + dashed line for average

Do SWAG.



OWN questions/comments from reading through code:

In swag_inference:
 -Line 345, should pass something else du to ennumerate -> might break
 
 -Invert for loop of model and S samples to save space.

 -Try using LowRankMultivariateNormal instead of the regular one

 -
 Possible fixes for running out of memory:
  - We're not using no_grad maybe putting it makes it lighter
  - We're using .tar that apparently is a non-compressed file format. WEe could look into other ways
